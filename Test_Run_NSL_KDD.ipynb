{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb0r6mSgPR2Laz+/Y30Roo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isj0/DeepLearning/blob/main/Test_Run_NSL_KDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import ibraries and load dataset"
      ],
      "metadata": {
        "id": "3Up906Ny1cDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSzGzdLWwZJ2"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the NSL-KDD dataset from HuggingFace\n",
        "\n",
        "ds = load_dataset(\"Mireu-Lab/NSL-KDD\")"
      ],
      "metadata": {
        "id": "lcdBCYd-x-xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face datasets need to be converted to pandas DataFrame for easy handling\n",
        "\n",
        "# convert train and test splits to dataFrames\n",
        "train_df = pd.DataFrame(ds['train'])\n",
        "test_df = pd.DataFrame(ds['test'])"
      ],
      "metadata": {
        "id": "LdXSGhKhzAng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore dataset"
      ],
      "metadata": {
        "id": "9fysFZ8m1mPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preview a few first rows of the data\n",
        "\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "3zJXiXeC0Oee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display shape of datasets\n",
        "\n",
        "print(\"Training dataset shape:\", train_df.shape)\n",
        "print(\"Testing dataset shape:\", test_df.shape)"
      ],
      "metadata": {
        "id": "AVXBJJy2z0kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the column names\n",
        "\n",
        "print(\"Columns in the dataset:\")\n",
        "print(train_df.columns)\n"
      ],
      "metadata": {
        "id": "Z-lGdwM30Jos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types of each column\n",
        "print(\"Data types of each column:\")\n",
        "print(train_df.dtypes)"
      ],
      "metadata": {
        "id": "YB0EEyQG0V-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check target class distribution\n",
        "\n",
        "print(\"Attack types in training dataset:\")\n",
        "print(train_df['class'].value_counts())"
      ],
      "metadata": {
        "id": "Unq9V1rO0cUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "mC0A_xrK1qEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicate rows in training and test datasets\n",
        "\n",
        "print(\"Number of duplicate rows in training set:\", train_df.duplicated().sum())\n",
        "print(\"Number of duplicate rows in test set:\", test_df.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "vWHJVR8z09X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows\n",
        "\n",
        "train_df = train_df.drop_duplicates()\n",
        "test_df = test_df.drop_duplicates()\n",
        "\n",
        "# check for duplicate rows after removal\n",
        "print(\"Number of duplicate rows in training set:\", train_df.duplicated().sum())\n",
        "print(\"Number of duplicate rows in test set:\", test_df.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "qTn3mAgU2Dl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any missing values\n",
        "\n",
        "print(\"Missing values in training set:\\n\", train_df.isnull().sum())\n",
        "print(\"Missing values in test set:\\n\", test_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "w0vvL11d2GxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the target column to numeric: normal=0, anomaly=1\n",
        "\n",
        "train_df['label'] = train_df['class'].apply(lambda x: 0 if x=='normal' else 1)\n",
        "test_df['label'] = test_df['class'].apply(lambda x: 0 if x=='normal' else 1)\n",
        "\n",
        "# Now drop original class column\n",
        "train_df.drop('class', axis=1, inplace=True)\n",
        "test_df.drop('class', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "cTpBx5uB2RCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "HuvvCnGY2ybw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# peform One-hot encoding for categorical features\n",
        "\n",
        "categorical_features = ['protocol_type', 'service', 'flag']\n",
        "\n",
        "# convert categorical features to one-hot encoded columns\n",
        "train_df_encoded = pd.get_dummies(train_df, columns=categorical_features)\n",
        "test_df_encoded = pd.get_dummies(test_df, columns=categorical_features)\n"
      ],
      "metadata": {
        "id": "bPdJtXfn2bQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# align train and test after one-hot encoding\n",
        "\n",
        "train_df_encoded, test_df_encoded = train_df_encoded.align(test_df_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# now confirm they have the same number of columns\n",
        "\n",
        "print(\"Training features shape:\", train_df_encoded.shape)\n",
        "print(\"Testing features shape:\", test_df_encoded.shape)\n"
      ],
      "metadata": {
        "id": "6-M235BT5qqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numeric features\n",
        "\n",
        "# identify numeric columns\n",
        "numeric_cols = train_df.select_dtypes(include=np.number).columns.tolist()\n",
        "# remove target column from features\n",
        "numeric_cols.remove('label')\n",
        "\n",
        "print(\"Numeric columns to scale:\", numeric_cols)\n"
      ],
      "metadata": {
        "id": "k9c1clfg235M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit the scaler on training data numeric columns and transform\n",
        "train_df_encoded[numeric_cols] = scaler.fit_transform(train_df_encoded[numeric_cols])\n",
        "\n",
        "# transform the test data using the same scaler\n",
        "test_df_encoded[numeric_cols] = scaler.transform(test_df_encoded[numeric_cols])\n",
        "\n",
        "# check a few rows to see scaled numeric features\n",
        "print(\"Numeric features scaled. Sample data:\")\n",
        "print(train_df_encoded[numeric_cols].head())"
      ],
      "metadata": {
        "id": "Q6IWmwa8hAi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply PCA to reduce dimensionality\n",
        "# we will keep enough components to retain 95% of variance\n",
        "\n",
        "# separate features and target\n",
        "X_train = train_df_encoded.drop('label', axis=1)\n",
        "y_train = train_df_encoded['label']\n",
        "\n",
        "X_test = test_df_encoded.drop('label', axis=1)\n",
        "y_test = test_df_encoded['label']\n",
        "\n",
        "# Initialize PCA to retain 95% of variance\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "\n",
        "# fit PCA on training features and transform\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# transform test features using the same PCA\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# check how many components PCA kept\n",
        "print(\"Original number of features:\", X_train.shape[1])\n",
        "print(\"Reduced number of features after PCA:\", X_train_pca.shape[1])\n"
      ],
      "metadata": {
        "id": "SsLG8CFT5Eq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to compute, store and key evaluation metrics.\n",
        "\n",
        "# dictionary to store results\n",
        "model_results = {}\n",
        "\n",
        "def evaluation_metrics(y_true, y_pred, model_name=\"Model\"):\n",
        "\n",
        "    # compute metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    # print metrics\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"Accuracy : {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"F1-score : {f1:.4f}\")\n",
        "\n",
        "    # store metrics in the global dictionary\n",
        "    model_results[model_name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1\": f1\n",
        "    }\n",
        "\n",
        "    # return metrics dictionary\n",
        "    return model_results[model_name]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VGDXLfeREley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to plot a confusion matrix as a heatmap for our models\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\"):\n",
        "\n",
        "    # compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # plotting\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UNaroediF3a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# initialize the model\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)  # max_iter increased to ensure convergence\n",
        "\n",
        "# fit the model on PCA-transformed training data\n",
        "logreg.fit(X_train_pca, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred_logreg = logreg.predict(X_test_pca)\n",
        "\n",
        "# evaluate metrics\n",
        "evaluation_metrics(y_test, y_pred_logreg, model_name=\"Logistic Regression\")\n",
        "\n",
        "# plot confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_logreg, model_name=\"Logistic Regression\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vTUbRpQhJzj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "\n",
        "# initialize the Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# fit the model on PCA-transformed training data\n",
        "dt.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_dt = dt.predict(X_test_pca)\n",
        "\n",
        "# evaluate metrics\n",
        "evaluation_metrics(y_test, y_pred_dt, model_name=\"Decision Tree\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_dt, model_name=\"Decision Tree\")\n"
      ],
      "metadata": {
        "id": "aJislaNQLMg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "\n",
        "# initialize the Random Forest\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# fit the model on PCA-transformed training data\n",
        "rf.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf = rf.predict(X_test_pca)\n",
        "\n",
        "# evaluate metrics\n",
        "evaluation_metrics(y_test, y_pred_rf, model_name=\"Random Forest\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_rf, model_name=\"Random Forest\")\n"
      ],
      "metadata": {
        "id": "R446kuR4MLmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector machine\n",
        "\n",
        "# initialize the SVM with RBF kernel\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# train the model on PCA-transformed training data\n",
        "svm_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred_svm = svm_model.predict(X_test_pca)\n",
        "\n",
        "# evaluate metrics\n",
        "evaluation_metrics(y_test, y_pred_svm, model_name=\"SVM\")\n",
        "\n",
        "# plot confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_svm, model_name=\"SVM\")\n"
      ],
      "metadata": {
        "id": "XcROwSyDM2Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "# initialize MLP\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=200, random_state=42)\n",
        "\n",
        "# train MLP\n",
        "mlp_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_mlp = mlp_model.predict(X_test_pca)\n",
        "\n",
        "# Evaluate metrics\n",
        "evaluation_metrics(y_test, y_pred_mlp, model_name=\"MLP\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_mlp, model_name=\"MLP\")\n"
      ],
      "metadata": {
        "id": "3hKRqXEJNYbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA reduces dimensionality\n",
        "\n",
        "You went from 122 features → 24 features.\n",
        "\n",
        "PCA keeps ~95% of the variance, but some information is inevitably lost.\n",
        "\n",
        "This can affect models like Random Forest, which can handle many features and might benefit from the full feature set.\n",
        "\n",
        "Random Forest doesn’t require PCA\n",
        "\n",
        "RF is tree-based, not linear, so it can naturally handle correlated or high-dimensional features.\n",
        "\n",
        "PCA is more useful for linear models (like Logistic Regression, SVM with RBF) where too many correlated features can hurt performance or increase computation.\n",
        "\n",
        "Compare results\n",
        "\n",
        "Training RF on PCA features gave you one set of metrics.\n",
        "\n",
        "Training RF on original features gives a baseline to see if PCA helped or hurt.\n",
        "\n",
        "You might find that RF on original features has better recall or F1-score, especially for the minority class (anomalies)."
      ],
      "metadata": {
        "id": "BIV-wOPBvSzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Hyperparameter Tuning using RandomizedSearchCV\n",
        "\n",
        "# create a small set of parameters to test\n",
        "rf_param_grid_fast = {\n",
        "    \"n_estimators\": [50, 100, 200],       # num of trees\n",
        "    \"max_depth\": [None, 10, 20],          # max depth of each tree\n",
        "    \"min_samples_split\": [2, 5, 10],      # min samples to split a node\n",
        "    \"min_samples_leaf\": [1, 2, 4],        # min samples at leaf\n",
        "    \"max_features\": [\"sqrt\", 0.5],        # features considered per split\n",
        "    \"bootstrap\": [True]                    # use bootstrap samples\n",
        "}\n",
        "\n",
        "# create a random forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# set up RandomizedSearchCV to find best parameters\n",
        "rf_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=rf_param_grid_fast,\n",
        "    n_iter=10,          # try only 10 random combinations\n",
        "    cv=3,               # 3-fold cross validation\n",
        "    scoring='f1',       # focus on F1 due to class imbalance\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1           # use all CPU cores\n",
        ")\n",
        "\n",
        "# Train the model and measure time\n",
        "start_time = time.time()\n",
        "rf_search.fit(X_train_pca, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Random Forest tuning completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Show the best hyperparameters\n",
        "print(\"\\nBest Random Forest Hyperparameters:\")\n",
        "print(rf_search.best_params_)\n",
        "\n",
        "# predict on test data\n",
        "y_pred_rf_best = rf_search.best_estimator_.predict(X_test_pca)\n",
        "\n",
        "# evaluation metrics and confusion matrix\n",
        "evaluation_metrics(y_test, y_pred_rf_best, model_name=\"Random Forest (Tuned simple)\")\n",
        "plot_confusion_matrix(y_test, y_pred_rf_best, model_name=\"Random Forest (Tuned simple)\")\n"
      ],
      "metadata": {
        "id": "xxloJjVTRPLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest on all original features (No PCA)\n",
        "\n",
        "print(\"Training Random Forest on original features\")\n",
        "\n",
        "rf_no_pca = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# train random forest on all features\n",
        "rf_no_pca.fit(X_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred_rf_no_pca = rf_no_pca.predict(X_test)\n",
        "\n",
        "# evaluation\n",
        "evaluation_metrics(y_test, y_pred_rf_no_pca, model_name=\"Random Forest (No PCA)\")\n",
        "plot_confusion_matrix(y_test, y_pred_rf_no_pca, model_name=\"Random Forest (No PCA)\")\n"
      ],
      "metadata": {
        "id": "-zJ551xr6yM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM hyperparameter tuning setup using RandomizedSearchCV:\n",
        "# We want to find the best SVM settings for our data\n",
        "\n",
        "# choose some SVM options to try\n",
        "svm_options = {\n",
        "    'C': [0.1, 1, 10, 50],         # penalty\n",
        "    'gamma': ['scale', 0.01, 0.1],\n",
        "    'kernel': ['rbf']              # RBF kernel\n",
        "}\n",
        "\n",
        "# create the svm model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# try different settings\n",
        "# RandomizedSearchCV will try 10 combinations of the options with 3-fold CV\n",
        "\n",
        "svm_search = RandomizedSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_distributions=svm_options,\n",
        "    n_iter=10,        # try 10 random combinations\n",
        "    cv=3,             # 3-fold cross-validation\n",
        "    scoring='f1',       # focus on F1-score due to class imbalance\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1         # use all CPU cores\n",
        ")\n",
        "\n",
        "# fit with PCA reduced training data\n",
        "start_time = time.time()\n",
        "svm_search.fit(X_train_pca, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"SVM tuning completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# see which setting worked\n",
        "best_svm = svm_search.best_estimator_\n",
        "print(\"\\nBest SVM Hyperparameters found:\")\n",
        "print(svm_search.best_params_)\n",
        "\n",
        "# Test the best SVM on our test data\n",
        "y_pred_svm = best_svm.predict(X_test_pca)\n",
        "\n",
        "# evaluations\n",
        "evaluation_metrics(y_test, y_pred_svm, model_name=\"SVM (Tuned)\")\n",
        "plot_confusion_matrix(y_test, y_pred_svm, model_name=\"SVM (Tuned)\")"
      ],
      "metadata": {
        "id": "k2Qmx5oOLEh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Classifier\n",
        "\n",
        "# create the model\n",
        "gb = GradientBoostingClassifier(\n",
        "    # number of trees\n",
        "    n_estimators=150,\n",
        "    # learning rate\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on PCA-reduced training data\n",
        "gb.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_gb = gb.predict(X_test_pca)\n",
        "\n",
        "# evaluation\n",
        "evaluation_metrics(y_test, y_pred_gb, model_name=\"Gradient Boosting\")\n",
        "plot_confusion_matrix(y_test, y_pred_gb, model_name=\"Gradient Boosting\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WXebDzn4AWfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolation Forest (Unsupervised model)\n",
        "\n",
        "# create the model\n",
        "iso = IsolationForest(\n",
        "    # num of trees\n",
        "    n_estimators=200,\n",
        "    contamination=y_train.mean(),  # ratio of attacks in data\n",
        "    max_samples='auto',         # samples each tree uses\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model using features only -no labels\n",
        "iso.fit(X_train)\n",
        "\n",
        "# predict on test data (-1 = anomaly, 1 = normal)\n",
        "y_pred_iso = iso.predict(X_test)\n",
        "\n",
        "# Convert the output to 0 (normal) / 1 (attack)\n",
        "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)\n",
        "\n",
        "# evaluate performace\n",
        "evaluation_metrics(y_test, y_pred_iso, model_name=\"Isolation Forest\")\n",
        "plot_confusion_matrix(y_test, y_pred_iso, model_name=\"Isolation Forest\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Wx5bihqjDAJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid Ensemble: Gradient Boosting + Isolation Forest\n",
        "\n",
        "# supervised model predictions - gradient boosting\n",
        "gb_pred = y_pred_gb\n",
        "\n",
        "# Unsupervised model predictions - Isolation Forest\n",
        "iso_pred = y_pred_iso\n",
        "\n",
        "# Ensemble prediction using OR rule\n",
        "ensemble_pred = ((gb_pred == 1) | (iso_pred == 1)).astype(int)\n",
        "\n",
        "# evaluate ensemble\n",
        "evaluation_metrics(y_test, ensemble_pred, model_name=\"Hybrid Ensemble\")\n",
        "plot_confusion_matrix(y_test, ensemble_pred, model_name=\"Hybrid Ensemble\")\n"
      ],
      "metadata": {
        "id": "kewxuTTRDPta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional ensemble variations - AND-rule Ensemble\n",
        "\n",
        "ensemble_and = ((gb_pred == 1) & (iso_pred == 1)).astype(int)\n",
        "\n",
        "# evaluate AND-rule ensemble\n",
        "evaluation_metrics(y_test, ensemble_and, model_name=\"Hybrid Ensemble (AND)\")\n",
        "plot_confusion_matrix(y_test, ensemble_and, model_name=\"Hybrid Ensemble (AND)\")\n"
      ],
      "metadata": {
        "id": "5pMvCSi7LaiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Majority Voting Ensemble\n",
        "# Here we combine predictions from three models:\n",
        "# Gradient Boosting (supervised)\n",
        "# Isolation Forest (unsupervised)\n",
        "# Random Forest (supervised, no PCA version)\n",
        "\n",
        "\n",
        "# Stack predictions into a matrix: rows = samples, columns = models\n",
        "preds = np.vstack([gb_pred, iso_pred, y_pred_rf_no_pca]).T\n",
        "\n",
        "# now apply majority voting across the three models\n",
        "ensemble_majority = mode(preds, axis=1)[0].flatten()\n",
        "\n",
        "# Evaluate Majority voting ensemble\n",
        "evaluation_metrics(y_test, ensemble_majority, model_name=\"Hybrid Ensemble (Majority Voting)\")\n",
        "plot_confusion_matrix(y_test, ensemble_majority, model_name=\"Hybrid Ensemble (Majority Voting)\")\n"
      ],
      "metadata": {
        "id": "a0frKclELvpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the classification report\n",
        "print(\"Classification Report for Hybrid Ensemble (OR-rule)\")\n",
        "print(classification_report(y_test, ensemble_pred, target_names=['Normal', 'Attack']))"
      ],
      "metadata": {
        "id": "c33DotDIM5dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize all model metrics in a table\n",
        "\n",
        "# convert the model_results dictionary into a dataframe\n",
        "\n",
        "results_df = pd.DataFrame(model_results).T\n",
        "\n",
        "# sort the models by F1-score\n",
        "results_df = results_df.sort_values(by=\"F1\", ascending=False)\n",
        "\n",
        "display(results_df.style.background_gradient(cmap=\"Blues\").format(\"{:.4f}\"))"
      ],
      "metadata": {
        "id": "uDiRaH1IEFDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(results_df.index, results_df['F1'], color='skyblue')\n",
        "plt.ylabel('F1-score')\n",
        "plt.title('Comparison of Model Performance (F1-score)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AHvHXG4lbH0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot all metrics as grouped bars\n",
        "ax = results_df[['Accuracy', 'Precision', 'Recall', 'F1']].plot(\n",
        "    kind='bar', figsize=(12,6), width=0.8)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('All Model Metrics Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)  # all metrics are between 0 and 1\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mRlAwDhGcQLv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}